{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44t6IvfiQJ4z"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Lambda, Dropout, BatchNormalization\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import urllib\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_file_path = '/content/archive (2).zip'"
      ],
      "metadata": {
        "id": "UzLbzKl2YICP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Datanew='/content/Datanew'\n",
        "# Create a ZipFile object\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    # Extract all the contents to the target folder\n",
        "    zip_ref.extractall(Datanew)\n",
        "\n",
        "print(\"Extraction complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnnj5AJHYYGT",
        "outputId": "660d769f-10ad-447f-a9dd-334bd41ea153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "folder_path = '/content/Datanew'\n",
        "extensions_to_keep = ['.jpg', '.png', '.gif']  # Add the extensions you want to keep\n",
        "\n",
        "# Delete files that are not inside a folder and do not have specified extensions\n",
        "for file in os.listdir(folder_path):\n",
        "    file_path = os.path.join(folder_path, file)\n",
        "\n",
        "    # Check if the file is a regular file (not a directory) in the root of the specified folder\n",
        "    if os.path.isfile(file_path):\n",
        "        # Check if the file has an extension and if it's not in the list of extensions to keep\n",
        "        if '.' in file and not any(file.lower().endswith(ext) for ext in extensions_to_keep):\n",
        "            os.remove(file_path)\n",
        "\n",
        "print(\"Cleanup complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgjDS50VYriB",
        "outputId": "a0c1ea8c-edf6-4dc5-9f66-646bfa71ae06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleanup complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a list of all subdirectories in the specified path\n",
        "subdirs = [d for d in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, d))]"
      ],
      "metadata": {
        "id": "JGGjbFbZZZn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(subdirs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZ3lC0jJZviB",
        "outputId": "0264a68c-6a03-4a1e-86f9-490d961587a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sign_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/Datanew/sign_data/train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBBz4rpEZyLv",
        "outputId": "a0d7cfce-2d1e-45b1-8e16-a06bb4c1c221"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "001\t  012_forg  019       025_forg\t032\t  038_forg  045       051_forg\t058\t  064_forg\n",
            "001_forg  013\t    019_forg  026\t032_forg  039\t    045_forg  052\t058_forg  065\n",
            "002\t  013_forg  020       026_forg\t033\t  039_forg  046       052_forg\t059\t  065_forg\n",
            "002_forg  014\t    020_forg  027\t033_forg  040\t    046_forg  053\t059_forg  066\n",
            "003\t  014_forg  021       027_forg\t034\t  040_forg  047       053_forg\t060\t  066_forg\n",
            "003_forg  015\t    021_forg  028\t034_forg  041\t    047_forg  054\t060_forg  067\n",
            "004\t  015_forg  022       028_forg\t035\t  041_forg  048       054_forg\t061\t  067_forg\n",
            "004_forg  016\t    022_forg  029\t035_forg  042\t    048_forg  055\t061_forg  068\n",
            "006\t  016_forg  023       029_forg\t036\t  042_forg  049       055_forg\t062\t  068_forg\n",
            "006_forg  017\t    023_forg  030\t036_forg  043\t    049_forg  056\t062_forg  069\n",
            "009\t  017_forg  024       030_forg\t037\t  043_forg  050       056_forg\t063\t  069_forg\n",
            "009_forg  018\t    024_forg  031\t037_forg  044\t    050_forg  057\t063_forg\n",
            "012\t  018_forg  025       031_forg\t038\t  044_forg  051       057_forg\t064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/Datanew/sign_data'\n",
        "train_dir = os.path.join(data_dir, 'train')"
      ],
      "metadata": {
        "id": "_YbxIju6bVAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import image_dataset_from_directory # Import from the correct module\n",
        "\n",
        "# Load the training data\n",
        "train_dataset = image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    subset='training',\n",
        "    validation_split=0.2,\n",
        "    seed=123,\n",
        "    image_size=(224, 224),\n",
        "    batch_size=32\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbKfS100bgK3",
        "outputId": "80aab53b-ef24-4bf2-8701-6643026cf1c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1649 files belonging to 128 classes.\n",
            "Using 1320 files for training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation"
      ],
      "metadata": {
        "id": "agRLFGbBceit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_data_dir = '/content/Datanew/sign_data_augmented'\n",
        "new_train_dir = os.path.join(new_data_dir, 'train')\n",
        "os.makedirs(new_data_dir, exist_ok=True)\n",
        "os.makedirs(new_train_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "L7Od_Q_2crD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the training data\n",
        "train_dataset = image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    subset='training',\n",
        "    validation_split=0.2,\n",
        "    seed=123,\n",
        "    image_size=(224, 224),\n",
        "    batch_size=32\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGMclHX_o04v",
        "outputId": "759811da-18c6-499a-b814-03cfd8148653"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1649 files belonging to 128 classes.\n",
            "Using 1320 files for training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the data augmentation transformations\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
        "    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "    tf.keras.layers.experimental.preprocessing.RandomZoom(0.2),\n",
        "    tf.keras.layers.experimental.preprocessing.RandomContrast(0.2)\n",
        "])"
      ],
      "metadata": {
        "id": "JfIDH2QBo21X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the data with data augmentation and save the augmented images\n",
        "def augment_and_preprocess(image, label):\n",
        "    image = tf.cast(image, tf.float32)\n",
        "\n",
        "    # Get the subdirectory name\n",
        "    subdir = os.path.dirname(train_dataset.filenames[label.numpy()[0]])\n",
        "\n",
        " # Create the subdirectory in the new directory\n",
        "    new_subdir = os.path.join(new_train_dir, subdir) # Define new_subdir within the function\n",
        "    os.makedirs(new_subdir, exist_ok=True)\n",
        "\n",
        "    # Save the original image\n",
        "    tf.keras.preprocessing.image.save_img(\n",
        "        os.path.join(new_subdir, f\"{os.path.basename(train_dataset.filenames[label.numpy()[0]])}.png\"), image / 255.0\n",
        "    )"
      ],
      "metadata": {
        "id": "VnYcN7xdpGRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine tuning on new data"
      ],
      "metadata": {
        "id": "6pPTmaX1vdYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if os.path.exists('/content/resnet50_weights.h5'):\n",
        "    print(\"File exists\")\n",
        "else:\n",
        "    print(\"File not found\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2pDHAkpwr2_",
        "outputId": "05d1f81d-5ba7-4a41-9c2e-4bc7117f1820"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ohR5Epp0xAmF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}